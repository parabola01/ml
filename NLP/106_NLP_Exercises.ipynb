{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "['Here we go again.', 'I was supposed to add this text later.', \"Well, it's 10.p.m. here, and I'm actually having fun making this course.\", ':oI hope you are getting along fine with this presentation, I really did try.', 'And one last sentence, just so you can test you tokenizers better.']\n",
      "Tokenized words:\n",
      "['Here', 'we', 'go', 'again.', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later.', 'Well,', \"it's\", '10.p.m.', 'here,', 'and', \"I'm\", 'actually', 'having', 'fun', 'making', 'this', 'course.', ':oI', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation,', 'I', 'really', 'did', 'try.', 'And', 'one', 'last', 'sentence,', 'just', 'so', 'you', 'can', 'test', 'you', 'tokenizers', 'better.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import re\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    return re.split('\\\\s+', text)\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "    \"\"\"Tokenize text into words using regex.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "            Text to be tokenized\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "            List containing words tokenized from text\n",
    "\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s', text)\n",
    "    return sentences\n",
    "text = \"Here we go again. I was supposed to add this text later. \\\n",
    "Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    "I hope you are getting along fine with this presentation, I really did try. \\\n",
    "And one last sentence, just so you can test you tokenizers better.\"\n",
    "\n",
    "print(\"Tokenized sentences:\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"Tokenized words:\")\n",
    "print(tokenize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank: NNP\n",
      "you: PRP\n",
      "very: RB\n",
      "much: RB\n",
      "Mr.: NNP\n",
      "Speaker: NNP\n",
      "Mr.: NNP\n",
      "Vice: NNP\n",
      "President: NNP\n",
      "Members: NNP\n",
      "of: IN\n",
      "Congress: NNP\n",
      "the: DT\n",
      "First: NNP\n",
      "Lady: NNP\n",
      "of: IN\n",
      "the: DT\n",
      "United: NNP\n",
      "States: NNPS\n",
      "and: CC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ola55\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ola55\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "file_path = \"../datasets/trump.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    trump = file.read()\n",
    "\n",
    "words = word_tokenize(trump)\n",
    "\n",
    "words = [word for word in words if word not in string.punctuation]\n",
    "\n",
    "word_tags = pos_tag(words)\n",
    "\n",
    "for word, tag in word_tags[:20]:\n",
    "    print(f\"{word}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['vision', 'years', 'freedom', 'tonight', 'chapter', 'greatness']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['time', 'thinking']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['time', 'fights']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['courage', 'dreams', 'hearts', 'bravery', 'hopes', 'souls', 'confidence', 'hopes', 'dreams', 'action']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['aspirations', 'fears', 'future', 'failures', 'past', 'vision', 'doubts']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['citizens', 'renewal', 'spirit']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['Members', 'things', 'country']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['tonight', 'moment']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "['yourselves', 'future']\n",
      "Sentence: Thank you, God bless you, and God bless the United States.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "file_path = \"../datasets/trump.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    trump = file.read()\n",
    "\n",
    "doc = nlp(trump)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "last_10_sentences = sentences[-10:]\n",
    "\n",
    "nouns_by_sentence = []\n",
    "for sentence in last_10_sentences:\n",
    "    nouns = [token.text for token in sentence if token.pos_ == \"NOUN\"]\n",
    "    nouns_by_sentence.append(nouns)\n",
    "\n",
    "for i, nouns in enumerate(nouns_by_sentence, 1):\n",
    "    print(f'Sentence: {str(sentence).rstrip()}')\n",
    "    print(f\"{nouns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words matrix:\n",
      "[[1 1 1 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 1 0 1 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 1 0 0]]\n",
      "\n",
      "Feature names:\n",
      "['bag', 'base', 'count', 'document', 'give', 'matrix', 'multiple', 'occur', 'occurrence', 'pretty', 'sparse', 'word', 'words']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation using spaCy for text processing.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.__bow_list = []\n",
    "\n",
    "    def fit_transform(self, corpus: list) -> np.array:\n",
    "        \"\"\"Transform list of strings into BoW array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: List[str]\n",
    "                Corpus of texts to be transformed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "                Matrix representation of BoW\n",
    "        \"\"\"\n",
    "        # Process the documents and build the vocabulary\n",
    "        docs_tokens = self.__process_documents(corpus)\n",
    "        self.__build_vocabulary(docs_tokens)\n",
    "\n",
    "        # Create and return the BoW matrix\n",
    "        return self.__create_bow_matrix(docs_tokens)\n",
    "\n",
    "    def get_feature_names(self) -> list:\n",
    "        \"\"\"Return words corresponding to columns of matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "                Words being transformed by fit function\n",
    "        \"\"\"\n",
    "        return self.__bow_list\n",
    "\n",
    "    def __process_documents(self, corpus: list) -> list:\n",
    "        \"\"\"Tokenize and preprocess the documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus: List[str]\n",
    "                Corpus of texts to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[List[str]]\n",
    "                Tokenized and preprocessed documents\n",
    "        \"\"\"\n",
    "        docs_tokens = []\n",
    "        for doc in corpus:\n",
    "            tokens = [token.lemma_.lower() for token in self.__nlp(doc)\n",
    "                      if not token.is_punct and not token.is_stop]\n",
    "            docs_tokens.append(tokens)\n",
    "        return docs_tokens\n",
    "\n",
    "    def __build_vocabulary(self, docs_tokens: list):\n",
    "        \"\"\"Build the vocabulary from the tokenized documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        docs_tokens: List[List[str]]\n",
    "                     Tokenized and preprocessed documents\n",
    "        \"\"\"\n",
    "        unique_tokens = set(token for doc_tokens in docs_tokens for token in doc_tokens)\n",
    "        self.__bow_list = sorted(unique_tokens)\n",
    "\n",
    "    def __create_bow_matrix(self, docs_tokens: list) -> np.array:\n",
    "        \"\"\"Create the BoW matrix from tokenized documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        docs_tokens: List[List[str]]\n",
    "                     Tokenized and preprocessed documents\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "                Matrix representation of BoW\n",
    "        \"\"\"\n",
    "        bow_matrix = np.zeros((len(docs_tokens), len(self.__bow_list)), dtype=np.int32)\n",
    "        for i, doc in enumerate(docs_tokens):\n",
    "            for token in doc:\n",
    "                bow_matrix[i, self.__bow_list.index(token)] += 1\n",
    "        return bow_matrix\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurrences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]\n",
    "\n",
    "\n",
    "bow = BagOfWords()\n",
    "matrix = bow.fit_transform(corpus)\n",
    "feature_names = bow.get_feature_names()\n",
    "\n",
    "print(\"Bag of Words matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nFeature names:\")\n",
    "print(feature_names)\n",
    "print(len(feature_names))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens\n",
    "\n",
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()\n",
    "\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_generated():\n",
    "    # put your code here\n",
    "    pass\n",
    "\n",
    "N=5 # fix it for other value of N\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We have\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "# put your code here:\n",
    "\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
